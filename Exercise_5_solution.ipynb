{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2trPuyzm9C"
      },
      "source": [
        "# Exercise 5.1 - Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipcsUFDUzm9C"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCJe_ITJzm9G"
      },
      "source": [
        "**Simple Network**\n",
        "\n",
        "We continue with the dataset first encountered in the previous exercise. Please refer to the discussion there for an introduction to the data and the learning objective.\n",
        "\n",
        "Here, we manually implement a simple network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NopU99AT9G7s",
        "outputId": "e7502f3d-87e0-4614-ffce-47cbf07a68a3"
      },
      "source": [
        "# The code snippet below is responsible for downloading the dataset\n",
        "# - for example when running via Google Colab.\n",
        "#\n",
        "# You can also directly download the file using the link if you work\n",
        "# with a local setup (in that case, ignore the !wget)\n",
        "\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-27 19:08:52--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264426 (258K) [application/x-httpd-php]\n",
            "Saving to: ‘winequality-white.csv.2’\n",
            "\n",
            "winequality-white.c 100%[===================>] 258.23K   499KB/s    in 0.5s    \n",
            "\n",
            "2022-09-27 19:08:53 (499 KB/s) - ‘winequality-white.csv.2’ saved [264426/264426]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ONqeI5Uzm9H",
        "outputId": "030c1174-6396-43e6-c04f-0ac9ffc7dbd3"
      },
      "source": [
        "# Before working with the data, \n",
        "# we download and prepare all features\n",
        "\n",
        "# load all examples from the file\n",
        "data = np.genfromtxt('winequality-white.csv',delimiter=\";\",skip_header=1)\n",
        "\n",
        "print(\"data:\", data.shape)\n",
        "\n",
        "# Prepare for proper training\n",
        "np.random.shuffle(data) # randomly sort examples\n",
        "\n",
        "# the performance can be increased by normalizing the data per feature\n",
        "# Normalize\n",
        "mean = np.mean(data, axis=0)\n",
        "std = np.std(data, axis = 0)\n",
        "print(mean.shape)\n",
        "\n",
        "data = (data - mean)/std\n",
        "\n",
        "# take the first 3000 examples for training\n",
        "# (remember array slicing from last week)\n",
        "X_train = data[:3000,:11] # all features except last column\n",
        "y_train = data[:3000,11]  # quality column\n",
        "\n",
        "# and the remaining examples for testing\n",
        "X_test = data[3000:,:11] # all features except last column\n",
        "y_test = data[3000:,11] # quality column\n",
        "\n",
        "print(\"First example:\")\n",
        "print(\"Features:\", X_train[0])\n",
        "print(\"Quality:\", y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: (4898, 12)\n",
            "(12,)\n",
            "First example:\n",
            "Features: [ 0.527639   -0.18099175 -0.03463842  1.06645815  0.51395124  0.74634612\n",
            " -0.26734905  0.97058182 -0.58460464  0.527131   -0.41793512]\n",
            "Quality: 0.13787014016904114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwnyNHpzm9L"
      },
      "source": [
        "# Solutions\n",
        "\n",
        "The goal is to implement the training of a neural network with one input layer, one hidden layer, and one output layer using gradient descent. We first (below) define the matrices and initialise with random values. We need W, b, W' and b'. The shapes will be:\n",
        "  * W: (number of hidden nodes, number of inputs) named `W`\n",
        "  * b: (number of hidden nodes) named `b`\n",
        "  * W': (number of hidden nodes) named `Wp`\n",
        "  * b': (one) named `bp`\n",
        "\n",
        "Your tasks are:     \n",
        "   * Implement a forward pass of the network as `dnn` (see below)\n",
        "   * Implement a function that uses one data point to update the weights using gradient descent. You can follow the `update_weights` skeleton below\n",
        "   * Now you can use the code below (training loop and evaluation) to train the network for multiple data points and even over several epochs. Try to find a set of hyperparameters (number of nodes in the hidden layer, learning rate, number of training epochs) that gives stable results. What is the best result (as measured by the loss on the training sample) you can get?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBm-DkpLglYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd53cc9-229d-4018-f91d-65354800d19f"
      },
      "source": [
        "# Initialise weights with suitable random distributions\n",
        "hidden_nodes = 50 # number of nodes in the hidden layer\n",
        "n_inputs = 11 # input features in the dataset\n",
        "\n",
        "\n",
        "W = np.random.randn(hidden_nodes,11)*np.sqrt(2./n_inputs)\n",
        "# b = np.random.randn(hidden_nodes)*np.sqrt(2./n_inputs)\n",
        "b = np.zeros(hidden_nodes)\n",
        "Wp = np.random.randn(hidden_nodes)*np.sqrt(2./hidden_nodes)\n",
        "bp = np.zeros(1)\n",
        "\n",
        "print(W.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ5euq5nglYf"
      },
      "source": [
        "# You can use this implementation of the ReLu activation function\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOlgGIZPglYg"
      },
      "source": [
        "def dnn(x,W,b,Wp,bp):\n",
        "    # SOLUTION\n",
        "    # sum_i W'_ki*Relu(sum_j W_ij*x_j + b_i) + b'_k \n",
        "    return np.dot(Wp, relu(np.dot(W,x) + b)) + bp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iKDPKR7glYg"
      },
      "source": [
        "def update_weights(x,y, W, b, Wp, bp):\n",
        "    \n",
        "    lr = 0.00005\n",
        "\n",
        "    # SOLUTION\n",
        "\n",
        "    # Calculate the network output\n",
        "    phi = dnn(x,W,b,Wp,bp)\n",
        "\n",
        "    # Use the formulas derived to calculate the gradient for each of W,b,Wp,bp\n",
        "    delta_bp = 2 * (phi - y)\n",
        "    delta_Wp = 2 * (phi - y) * relu(np.dot(W,x) + b)\n",
        "    delta_b  = 2 * (phi - y) * Wp * np.heaviside(np.dot(W,x) + b, 0.5)\n",
        "    delta_W  = 2 * (phi - y) * np.outer(Wp * np.heaviside(np.dot(W,x) + b, 0.5), x)\n",
        "                \n",
        "    # Update the weights/bias following the rule:  X_new = X_old - learning_rate * gradient    \n",
        "    bp -= lr * delta_bp\n",
        "    Wp -= lr * delta_Wp\n",
        "    b  -= lr * delta_b\n",
        "    W  -= lr * delta_W\n",
        "    return W, b, Wp, bp  # it is cleaner to explicitely return the new weights. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4yIPZBglYh"
      },
      "source": [
        "# Training loop and evaluation below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d285Cw67glYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ecd9fcd-b561-4e20-e7ce-73906a3a5614"
      },
      "source": [
        "# The code below implements the training.\n",
        "# If you correctly implement  dnn and update_weights above, \n",
        "# you should not need to change anything below. \n",
        "# (apart from increasing the number of epochs)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# How many epochs to train\n",
        "n_epochs = 200\n",
        "\n",
        "# Loop over the epochs\n",
        "for ep in range(n_epochs):\n",
        "        \n",
        "    # Each epoch is a complete over the training data\n",
        "    for i in range(X_train.shape[0]):\n",
        "        \n",
        "        # pick one example\n",
        "        x = X_train[i]\n",
        "        y = y_train[i]\n",
        "\n",
        "        # use it to update the weights\n",
        "        W, b, Wp, bp = update_weights(x,y, W, b, Wp, bp)\n",
        "    \n",
        "    # Calculate predictions for the full training and testing sample\n",
        "    y_pred_train = [dnn(x,W,b,Wp,bp)[0] for x in X_train]\n",
        "    y_pred = [dnn(x,W,b,Wp,bp)[0] for x in X_test]\n",
        "\n",
        "    # Calculate aver loss / example over the epoch\n",
        "    train_loss = sum((y_pred_train-y_train)**2) / y_train.shape[0]\n",
        "    test_loss = sum((y_pred-y_test)**2) / y_test.shape[0] \n",
        "    \n",
        "    # print some information\n",
        "    print(\"Epoch:\",ep, \"Train Loss:\", train_loss, \"Test Loss:\", test_loss)\n",
        "    \n",
        "    # and store the losses for later use\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: 0.9918851072505133 Test Loss: 0.9437235263946622\n",
            "Epoch: 1 Train Loss: 0.8539353363070646 Test Loss: 0.8170544596491086\n",
            "Epoch: 2 Train Loss: 0.8098654285399208 Test Loss: 0.7787141101404811\n",
            "Epoch: 3 Train Loss: 0.7864232962142866 Test Loss: 0.7590572587870429\n",
            "Epoch: 4 Train Loss: 0.7700840459173142 Test Loss: 0.7455918718301999\n",
            "Epoch: 5 Train Loss: 0.7573779537115456 Test Loss: 0.7352238844408449\n",
            "Epoch: 6 Train Loss: 0.74697031649237 Test Loss: 0.7268195364361066\n",
            "Epoch: 7 Train Loss: 0.7381950792835916 Test Loss: 0.7197999530174213\n",
            "Epoch: 8 Train Loss: 0.7306565228947146 Test Loss: 0.7138166894737549\n",
            "Epoch: 9 Train Loss: 0.7241003580174357 Test Loss: 0.7086470925618911\n",
            "Epoch: 10 Train Loss: 0.718300241003529 Test Loss: 0.7040884345943976\n",
            "Epoch: 11 Train Loss: 0.7131303562696022 Test Loss: 0.7000284967978013\n",
            "Epoch: 12 Train Loss: 0.7085015436380218 Test Loss: 0.6964030081592022\n",
            "Epoch: 13 Train Loss: 0.7043211639917011 Test Loss: 0.6931453659592702\n",
            "Epoch: 14 Train Loss: 0.7005347310897386 Test Loss: 0.6902052980773943\n",
            "Epoch: 15 Train Loss: 0.6970828296409518 Test Loss: 0.6875426022198844\n",
            "Epoch: 16 Train Loss: 0.6939282454764031 Test Loss: 0.6851139877530196\n",
            "Epoch: 17 Train Loss: 0.6910333300412104 Test Loss: 0.6828827868830903\n",
            "Epoch: 18 Train Loss: 0.6883707886579146 Test Loss: 0.6808394059363901\n",
            "Epoch: 19 Train Loss: 0.6859098401917959 Test Loss: 0.6789703099589743\n",
            "Epoch: 20 Train Loss: 0.6836431809306176 Test Loss: 0.6772528774062095\n",
            "Epoch: 21 Train Loss: 0.6815337766996303 Test Loss: 0.6756603485122262\n",
            "Epoch: 22 Train Loss: 0.6795623028541485 Test Loss: 0.6741859272348306\n",
            "Epoch: 23 Train Loss: 0.6777148834590229 Test Loss: 0.6728057230852279\n",
            "Epoch: 24 Train Loss: 0.6759821680609314 Test Loss: 0.6715192829340728\n",
            "Epoch: 25 Train Loss: 0.6743516570800373 Test Loss: 0.670317618454419\n",
            "Epoch: 26 Train Loss: 0.6728212301934773 Test Loss: 0.6691939494136528\n",
            "Epoch: 27 Train Loss: 0.6713768267164044 Test Loss: 0.6681395134871149\n",
            "Epoch: 28 Train Loss: 0.6700120046510696 Test Loss: 0.6671484631655877\n",
            "Epoch: 29 Train Loss: 0.6687208410968477 Test Loss: 0.666221382128213\n",
            "Epoch: 30 Train Loss: 0.6674993381886462 Test Loss: 0.6653503364595422\n",
            "Epoch: 31 Train Loss: 0.6663359197378768 Test Loss: 0.6645303621354283\n",
            "Epoch: 32 Train Loss: 0.665230995340108 Test Loss: 0.6637552907097762\n",
            "Epoch: 33 Train Loss: 0.6641767208661299 Test Loss: 0.663016150871849\n",
            "Epoch: 34 Train Loss: 0.663173464258023 Test Loss: 0.6623142129145835\n",
            "Epoch: 35 Train Loss: 0.6622149702377108 Test Loss: 0.6616479949987877\n",
            "Epoch: 36 Train Loss: 0.6612992261870828 Test Loss: 0.6610182671243733\n",
            "Epoch: 37 Train Loss: 0.660424485949666 Test Loss: 0.6604185131725026\n",
            "Epoch: 38 Train Loss: 0.659586766898926 Test Loss: 0.659844732335506\n",
            "Epoch: 39 Train Loss: 0.6587856002040534 Test Loss: 0.6593004331516642\n",
            "Epoch: 40 Train Loss: 0.6580196810737633 Test Loss: 0.6587810497202211\n",
            "Epoch: 41 Train Loss: 0.6572863569666775 Test Loss: 0.658291432870593\n",
            "Epoch: 42 Train Loss: 0.6565824185127884 Test Loss: 0.6578283855203159\n",
            "Epoch: 43 Train Loss: 0.655906552921044 Test Loss: 0.6573888564936845\n",
            "Epoch: 44 Train Loss: 0.6552540819705677 Test Loss: 0.6569675424877307\n",
            "Epoch: 45 Train Loss: 0.6546254108403493 Test Loss: 0.6565684887566291\n",
            "Epoch: 46 Train Loss: 0.6540166269113249 Test Loss: 0.6561866394243742\n",
            "Epoch: 47 Train Loss: 0.6534276451188153 Test Loss: 0.6558216159177876\n",
            "Epoch: 48 Train Loss: 0.6528593032978381 Test Loss: 0.6554710092754835\n",
            "Epoch: 49 Train Loss: 0.6523120473589672 Test Loss: 0.6551343637637064\n",
            "Epoch: 50 Train Loss: 0.6517792509223465 Test Loss: 0.6548098635664998\n",
            "Epoch: 51 Train Loss: 0.6512625368156081 Test Loss: 0.6544986166705571\n",
            "Epoch: 52 Train Loss: 0.6507631297248159 Test Loss: 0.6542010638209601\n",
            "Epoch: 53 Train Loss: 0.650278247769335 Test Loss: 0.6539138572355863\n",
            "Epoch: 54 Train Loss: 0.6498039749375243 Test Loss: 0.6536333666713656\n",
            "Epoch: 55 Train Loss: 0.6493421413062747 Test Loss: 0.65336368222802\n",
            "Epoch: 56 Train Loss: 0.6488911225974832 Test Loss: 0.6531010729463955\n",
            "Epoch: 57 Train Loss: 0.6484496954302063 Test Loss: 0.6528461609409129\n",
            "Epoch: 58 Train Loss: 0.6480175370450643 Test Loss: 0.6526003738256669\n",
            "Epoch: 59 Train Loss: 0.6475963542453279 Test Loss: 0.6523642358735272\n",
            "Epoch: 60 Train Loss: 0.647185465660025 Test Loss: 0.6521360536630618\n",
            "Epoch: 61 Train Loss: 0.6467835545231343 Test Loss: 0.6519156920823307\n",
            "Epoch: 62 Train Loss: 0.6463906300283166 Test Loss: 0.6516976464748889\n",
            "Epoch: 63 Train Loss: 0.646009388063929 Test Loss: 0.6514892757408147\n",
            "Epoch: 64 Train Loss: 0.6456416839121798 Test Loss: 0.6512906602273265\n",
            "Epoch: 65 Train Loss: 0.6452819417430035 Test Loss: 0.6510907105786539\n",
            "Epoch: 66 Train Loss: 0.6449318656997203 Test Loss: 0.6508968242661486\n",
            "Epoch: 67 Train Loss: 0.64458975298751 Test Loss: 0.6507121185365611\n",
            "Epoch: 68 Train Loss: 0.6442553454543866 Test Loss: 0.6505322152885195\n",
            "Epoch: 69 Train Loss: 0.643928173310011 Test Loss: 0.6503609298401758\n",
            "Epoch: 70 Train Loss: 0.6436054914347298 Test Loss: 0.6501891632651066\n",
            "Epoch: 71 Train Loss: 0.6432897912075083 Test Loss: 0.650021200841024\n",
            "Epoch: 72 Train Loss: 0.6429802955181856 Test Loss: 0.6498576485520678\n",
            "Epoch: 73 Train Loss: 0.6426746283287521 Test Loss: 0.6496965425042475\n",
            "Epoch: 74 Train Loss: 0.6423720913857769 Test Loss: 0.6495438635418688\n",
            "Epoch: 75 Train Loss: 0.6420773923576145 Test Loss: 0.6493960203150319\n",
            "Epoch: 76 Train Loss: 0.6417888290686552 Test Loss: 0.649249743486946\n",
            "Epoch: 77 Train Loss: 0.6415036633719117 Test Loss: 0.6491053831553519\n",
            "Epoch: 78 Train Loss: 0.6412246505174668 Test Loss: 0.6489647452763065\n",
            "Epoch: 79 Train Loss: 0.6409500645956161 Test Loss: 0.6488299938348923\n",
            "Epoch: 80 Train Loss: 0.6406810038864877 Test Loss: 0.6487006573123784\n",
            "Epoch: 81 Train Loss: 0.6404164659384505 Test Loss: 0.6485757889753588\n",
            "Epoch: 82 Train Loss: 0.6401561017435193 Test Loss: 0.6484561040667495\n",
            "Epoch: 83 Train Loss: 0.6398978850210006 Test Loss: 0.6483386981597972\n",
            "Epoch: 84 Train Loss: 0.6396444206468062 Test Loss: 0.6482239838652291\n",
            "Epoch: 85 Train Loss: 0.639393368013215 Test Loss: 0.6481112208192823\n",
            "Epoch: 86 Train Loss: 0.6391463927113759 Test Loss: 0.647999920291333\n",
            "Epoch: 87 Train Loss: 0.6389030137702635 Test Loss: 0.6478953752645138\n",
            "Epoch: 88 Train Loss: 0.638663967295462 Test Loss: 0.6477925590660701\n",
            "Epoch: 89 Train Loss: 0.6384261909855159 Test Loss: 0.6476889074441\n",
            "Epoch: 90 Train Loss: 0.6381931147399137 Test Loss: 0.6475920537867798\n",
            "Epoch: 91 Train Loss: 0.63796348893448 Test Loss: 0.6475038859204891\n",
            "Epoch: 92 Train Loss: 0.6377364813679838 Test Loss: 0.6474157100893975\n",
            "Epoch: 93 Train Loss: 0.6375114897686933 Test Loss: 0.6473273873409637\n",
            "Epoch: 94 Train Loss: 0.6372894869201836 Test Loss: 0.6472447932124182\n",
            "Epoch: 95 Train Loss: 0.6370687508781842 Test Loss: 0.6471641342146597\n",
            "Epoch: 96 Train Loss: 0.6368487311539004 Test Loss: 0.6470815878684132\n",
            "Epoch: 97 Train Loss: 0.636629616339908 Test Loss: 0.6470020710592951\n",
            "Epoch: 98 Train Loss: 0.6364098737884504 Test Loss: 0.6469224363391285\n",
            "Epoch: 99 Train Loss: 0.636192218280857 Test Loss: 0.6468428114278452\n",
            "Epoch: 100 Train Loss: 0.63597593037179 Test Loss: 0.6467640169754655\n",
            "Epoch: 101 Train Loss: 0.6357610696777313 Test Loss: 0.6466857655954328\n",
            "Epoch: 102 Train Loss: 0.6355475886698192 Test Loss: 0.6466076132023595\n",
            "Epoch: 103 Train Loss: 0.6353377170962112 Test Loss: 0.6465280201210054\n",
            "Epoch: 104 Train Loss: 0.635129396588202 Test Loss: 0.6464505642435212\n",
            "Epoch: 105 Train Loss: 0.6349220362003546 Test Loss: 0.6463706899647238\n",
            "Epoch: 106 Train Loss: 0.6347173726841154 Test Loss: 0.646292533483417\n",
            "Epoch: 107 Train Loss: 0.6345129106047935 Test Loss: 0.6462084213530002\n",
            "Epoch: 108 Train Loss: 0.6343106719183822 Test Loss: 0.6461296721700179\n",
            "Epoch: 109 Train Loss: 0.6341080776412966 Test Loss: 0.6460519657653498\n",
            "Epoch: 110 Train Loss: 0.6339084396624364 Test Loss: 0.6459745166547565\n",
            "Epoch: 111 Train Loss: 0.6337100677062371 Test Loss: 0.6458983784104226\n",
            "Epoch: 112 Train Loss: 0.6335111253672241 Test Loss: 0.6458174746351033\n",
            "Epoch: 113 Train Loss: 0.6333118636794055 Test Loss: 0.6457363749508974\n",
            "Epoch: 114 Train Loss: 0.6331118781624889 Test Loss: 0.6456539432188206\n",
            "Epoch: 115 Train Loss: 0.6329118063022228 Test Loss: 0.6455705083909135\n",
            "Epoch: 116 Train Loss: 0.6327117332308808 Test Loss: 0.6454906675432197\n",
            "Epoch: 117 Train Loss: 0.6325120712872877 Test Loss: 0.6454109691884717\n",
            "Epoch: 118 Train Loss: 0.6323177546495405 Test Loss: 0.6453360189805558\n",
            "Epoch: 119 Train Loss: 0.6321243591101364 Test Loss: 0.6452628263326753\n",
            "Epoch: 120 Train Loss: 0.6319334914056247 Test Loss: 0.6451898878543394\n",
            "Epoch: 121 Train Loss: 0.6317440899558717 Test Loss: 0.6451165036049815\n",
            "Epoch: 122 Train Loss: 0.6315551233039451 Test Loss: 0.6450438536819478\n",
            "Epoch: 123 Train Loss: 0.6313665204112393 Test Loss: 0.6449727728409965\n",
            "Epoch: 124 Train Loss: 0.631178476849953 Test Loss: 0.644901762677868\n",
            "Epoch: 125 Train Loss: 0.6309903157394265 Test Loss: 0.6448294435080204\n",
            "Epoch: 126 Train Loss: 0.630801199851477 Test Loss: 0.6447572090944689\n",
            "Epoch: 127 Train Loss: 0.6306147315127272 Test Loss: 0.6446868644613074\n",
            "Epoch: 128 Train Loss: 0.6304284834346277 Test Loss: 0.6446192424099956\n",
            "Epoch: 129 Train Loss: 0.6302409339280387 Test Loss: 0.6445487248595556\n",
            "Epoch: 130 Train Loss: 0.6300539288904373 Test Loss: 0.6444820183760136\n",
            "Epoch: 131 Train Loss: 0.6298674334239994 Test Loss: 0.6444154163166653\n",
            "Epoch: 132 Train Loss: 0.629680520678008 Test Loss: 0.6443467347752087\n",
            "Epoch: 133 Train Loss: 0.6294939210852717 Test Loss: 0.6442774145163341\n",
            "Epoch: 134 Train Loss: 0.6293077711610855 Test Loss: 0.6442065442292793\n",
            "Epoch: 135 Train Loss: 0.6291211673053263 Test Loss: 0.6441374172149402\n",
            "Epoch: 136 Train Loss: 0.6289343483456851 Test Loss: 0.6440674135757866\n",
            "Epoch: 137 Train Loss: 0.6287504576119989 Test Loss: 0.643997689085458\n",
            "Epoch: 138 Train Loss: 0.6285691202059444 Test Loss: 0.6439263266397343\n",
            "Epoch: 139 Train Loss: 0.628391282020249 Test Loss: 0.643858076930533\n",
            "Epoch: 140 Train Loss: 0.6282157982686852 Test Loss: 0.6437950674090229\n",
            "Epoch: 141 Train Loss: 0.6280415308608527 Test Loss: 0.643729700517236\n",
            "Epoch: 142 Train Loss: 0.6278686948413769 Test Loss: 0.643662181031518\n",
            "Epoch: 143 Train Loss: 0.6276949718307212 Test Loss: 0.6435923687585758\n",
            "Epoch: 144 Train Loss: 0.627522283520598 Test Loss: 0.6435219052879296\n",
            "Epoch: 145 Train Loss: 0.6273490245282937 Test Loss: 0.6434505242524431\n",
            "Epoch: 146 Train Loss: 0.6271756596564128 Test Loss: 0.6433830545025662\n",
            "Epoch: 147 Train Loss: 0.6270008568697477 Test Loss: 0.6433174267818456\n",
            "Epoch: 148 Train Loss: 0.6268277598852697 Test Loss: 0.6432528863756825\n",
            "Epoch: 149 Train Loss: 0.6266558804494169 Test Loss: 0.6431889829308908\n",
            "Epoch: 150 Train Loss: 0.6264864894798162 Test Loss: 0.6431235241509\n",
            "Epoch: 151 Train Loss: 0.6263181518774704 Test Loss: 0.6430580571300593\n",
            "Epoch: 152 Train Loss: 0.6261498246532875 Test Loss: 0.6429918816935355\n",
            "Epoch: 153 Train Loss: 0.6259827846331844 Test Loss: 0.6429241276141477\n",
            "Epoch: 154 Train Loss: 0.6258165049671384 Test Loss: 0.6428596776910341\n",
            "Epoch: 155 Train Loss: 0.6256501623546247 Test Loss: 0.6427930332504975\n",
            "Epoch: 156 Train Loss: 0.6254838297053924 Test Loss: 0.6427282221527681\n",
            "Epoch: 157 Train Loss: 0.6253184947766914 Test Loss: 0.6426622099627496\n",
            "Epoch: 158 Train Loss: 0.6251541402246028 Test Loss: 0.6425982111374844\n",
            "Epoch: 159 Train Loss: 0.6249908436254757 Test Loss: 0.6425384205662188\n",
            "Epoch: 160 Train Loss: 0.6248260681112869 Test Loss: 0.6424828232363312\n",
            "Epoch: 161 Train Loss: 0.6246628119278689 Test Loss: 0.6424282554960846\n",
            "Epoch: 162 Train Loss: 0.624501622331228 Test Loss: 0.6423709133652956\n",
            "Epoch: 163 Train Loss: 0.624340983370109 Test Loss: 0.6423138399791186\n",
            "Epoch: 164 Train Loss: 0.6241802968023464 Test Loss: 0.642254594169089\n",
            "Epoch: 165 Train Loss: 0.6240198401707737 Test Loss: 0.6421997194698321\n",
            "Epoch: 166 Train Loss: 0.6238615439725896 Test Loss: 0.6421487221386499\n",
            "Epoch: 167 Train Loss: 0.6237047126774838 Test Loss: 0.6420990740139412\n",
            "Epoch: 168 Train Loss: 0.6235497981617604 Test Loss: 0.642050812826467\n",
            "Epoch: 169 Train Loss: 0.6233931180988896 Test Loss: 0.6420086696686611\n",
            "Epoch: 170 Train Loss: 0.6232355913626487 Test Loss: 0.6419653833009594\n",
            "Epoch: 171 Train Loss: 0.6230778422360145 Test Loss: 0.6419183080143164\n",
            "Epoch: 172 Train Loss: 0.6229186777538034 Test Loss: 0.6418718763696569\n",
            "Epoch: 173 Train Loss: 0.622760130249509 Test Loss: 0.6418232290071222\n",
            "Epoch: 174 Train Loss: 0.6226014772991907 Test Loss: 0.6417747728674098\n",
            "Epoch: 175 Train Loss: 0.6224433042554739 Test Loss: 0.6417268727233325\n",
            "Epoch: 176 Train Loss: 0.6222860667367685 Test Loss: 0.6416801228486824\n",
            "Epoch: 177 Train Loss: 0.6221269611172543 Test Loss: 0.6416373383498246\n",
            "Epoch: 178 Train Loss: 0.6219687585567151 Test Loss: 0.6415929340753368\n",
            "Epoch: 179 Train Loss: 0.6218107894634394 Test Loss: 0.6415471682532321\n",
            "Epoch: 180 Train Loss: 0.6216527322641198 Test Loss: 0.641504224444927\n",
            "Epoch: 181 Train Loss: 0.6214964272912658 Test Loss: 0.6414598096431274\n",
            "Epoch: 182 Train Loss: 0.6213410492335553 Test Loss: 0.6414158822548229\n",
            "Epoch: 183 Train Loss: 0.6211881429842095 Test Loss: 0.6413683901398802\n",
            "Epoch: 184 Train Loss: 0.6210389232291473 Test Loss: 0.6413233067353018\n",
            "Epoch: 185 Train Loss: 0.6208914607662447 Test Loss: 0.6412807138514278\n",
            "Epoch: 186 Train Loss: 0.6207448428708138 Test Loss: 0.6412403395682512\n",
            "Epoch: 187 Train Loss: 0.6205976567072484 Test Loss: 0.6412021298738436\n",
            "Epoch: 188 Train Loss: 0.6204514295493845 Test Loss: 0.6411612963382444\n",
            "Epoch: 189 Train Loss: 0.6203042281865114 Test Loss: 0.6411219605905819\n",
            "Epoch: 190 Train Loss: 0.6201572426584162 Test Loss: 0.6410818744296217\n",
            "Epoch: 191 Train Loss: 0.6200113213559497 Test Loss: 0.6410456181214754\n",
            "Epoch: 192 Train Loss: 0.6198672787163553 Test Loss: 0.6410080241922999\n",
            "Epoch: 193 Train Loss: 0.6197264457759988 Test Loss: 0.6409696703327626\n",
            "Epoch: 194 Train Loss: 0.6195870612138435 Test Loss: 0.6409336771155345\n",
            "Epoch: 195 Train Loss: 0.6194481927814822 Test Loss: 0.6408950797865741\n",
            "Epoch: 196 Train Loss: 0.6193094874957623 Test Loss: 0.6408605446119096\n",
            "Epoch: 197 Train Loss: 0.6191702705760529 Test Loss: 0.6408242459059782\n",
            "Epoch: 198 Train Loss: 0.6190307600124622 Test Loss: 0.6407863127521128\n",
            "Epoch: 199 Train Loss: 0.6188926526119126 Test Loss: 0.6407456570044009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apz0-Y9xQPWb"
      },
      "source": [
        "# After the training:\n",
        "    \n",
        "# Prepare scatter plot\n",
        "y_pred = [dnn(x,W,b,Wp,bp)[0] for x in X_test]\n",
        "\n",
        "# now we need to rescale the output to the correct values\n",
        "y_pred = (y_pred + mean[11])* std[11]\n",
        "y_test = (y_test + mean[11])* std[11]\n",
        "y_pred_train = (y_pred_train + mean[11]) * std[11]\n",
        "y_train = (y_train + mean[11]) * std[11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best loss: {min(test_losses):.3f}, Final loss: {test_losses[-1]:.3f}\")\n",
        "\n",
        "print(f\"Correlation coefficient: {np.corrcoef(y_pred,y_test)[0,1]:.3f}\")\n",
        "plt.scatter(y_pred_train,y_train)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "h = ax.hist2d(y_pred_train, y_train, bins=[np.arange(0.5, 10.5, 1),np.arange(0.5, 10.5, 1)])\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_aspect(\"equal\")\n",
        "cbar = fig.colorbar(h[3], ax=ax)\n",
        "cbar.set_label(\"entries\")\n",
        "plt.show()\n",
        "\n",
        "# Prepare and loss over time\n",
        "plt.plot(train_losses[2:],label=\"train\")\n",
        "plt.plot(test_losses[2:],label=\"test\") # we omit the first data points as the first loss is typically very high which makes it difficult to read the plot. \n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "n561BnXnVoDG",
        "outputId": "2ab1e608-c786-467e-8c4b-a9d1167544f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best loss: 0.641, Final loss: 0.641\n",
            "Correlation coefficient: 0.578\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFNTC1SsuyuR",
        "outputId": "f3fe2249-3a64-461a-e60a-15e3cc7f85bb"
      },
      "source": [
        "print(np.max(np.abs(W)), np.average(np.abs(W))) # we see that single weights have a large value. Some regularization L2 norm might help to improve results. \n",
        "print(np.max(np.abs(Wp)), np.average(np.abs(Wp)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.762779690234602 0.3290683723553038\n",
            "0.45374070432121316 0.1288382275843679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkfAiy_glYi"
      },
      "source": [
        "# Hint 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMuB_2uFglYj"
      },
      "source": [
        "We want a network with one hidden layer. As activiation in the hidden layer $\\sigma$ we apply element-wise ReLu, while no activation is used for the output layer. The forward pass of the network then reads:\n",
        "$$\\hat{y}=\\mathbf{W}^{\\prime} \\sigma(\\mathbf{W} \\vec{x}+\\vec{b})+b^{\\prime}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3YzBLg2_DKF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubpT5nt4glYl"
      },
      "source": [
        "# Hint 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLtiXRNQglYl"
      },
      "source": [
        "For the regression problem the objective function is the mean squared error between the prediction and the true label $y$:\n",
        "$$\n",
        "L=(\\hat{y}-y)^{2}\n",
        "$$\n",
        "\n",
        "Taking the partial derivatives - and diligently applying chain rule - with respect to the different objects yields:\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial L}{\\partial b^{\\prime}} &=2(\\hat{y}-y) \\\\\n",
        "\\frac{\\partial L}{\\partial b_{k}} &=2(\\hat{y}-y) \\mathbf{W}_{k}^{\\prime} \\theta\\left(\\sum_{i} \\mathbf{W}_{k i} x_{i}+b_{k}\\right) \\\\\n",
        "\\frac{\\partial L}{\\partial \\mathbf{W}_{k}^{\\prime}} &=2(\\hat{y}-y) \\sigma\\left(\\sum_{i} \\mathbf{W}_{k i} x_{i}+b_{k}\\right) \\\\\n",
        "\\frac{\\partial L}{\\partial \\mathbf{W}_{k m}} &=2(\\hat{y}-y) \\mathbf{W}_{m}^{\\prime} \\theta\\left(\\sum_{i} \\mathbf{W}_{m i} x_{i}+b_{m}\\right) x_{k}\n",
        "\\end{aligned}\n",
        "\n",
        "Here, $\\Theta$ denotes the Heaviside step function."
      ]
    }
  ]
}