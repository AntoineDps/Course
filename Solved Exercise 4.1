{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14JL1QR6EwHDjZzvV7PpEQslEgxrgD6P2","timestamp":1663051952694},{"file_id":"https://github.com/DeepLearningForPhysicsResearchBook/deep-learning-physics/blob/main/Exercise_03_2.ipynb","timestamp":1629197629146}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rF2trPuyzm9C"},"source":["# Exercise 4\n"]},{"cell_type":"code","metadata":{"id":"ipcsUFDUzm9C","executionInfo":{"status":"ok","timestamp":1663231814014,"user_tz":-120,"elapsed":21,"user":{"displayName":"Antoine Dupuis","userId":"12168416720763859895"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCJe_ITJzm9G"},"source":["**Linear Regression**\n","\n","The goal of this exercise is to explore a simple linear regression problem based on Portugese white wine.\n","\n","The dataset is based on \n","Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. **Modeling wine preferences by data mining from physicochemical properties**. Published in Decision Support Systems, Elsevier, 47(4):547-553, 2009. \n","\n"]},{"cell_type":"code","metadata":{"id":"NopU99AT9G7s"},"source":["# The code snippet below is responsible for downloading the dataset\n","# - for example when running via Google Colab.\n","#\n","# You can also directly download the file using the link if you work\n","# with a local setup (in that case, ignore the !wget)\n","\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zEiZ19s5zm9G"},"source":["**Before we start**\n","\n","The downloaded file contains data on 4989 wines. For each wine 11 features are recorded (column 0 to 10). The final columns contains the quality of the wine. This is what we want to predict. More information on the features and the quality measurement is provided in the original publication.\n","\n","List of columns/features: \n","0. fixed acidity\n","1. volatile acidity\n","2. citric acid\n","3. residual sugar\n","4. chlorides\n","5. free sulfur dioxide\n","6. total sulfur dioxide\n","7. density\n","8. pH\n","9. sulphates\n","10. alcohol\n","11. quality\n","\n","\n","\n","[file]: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"]},{"cell_type":"code","metadata":{"id":"5ONqeI5Uzm9H"},"source":["# Before working with the data, \n","# we download and prepare all features\n","\n","# load all examples from the file\n","data = np.genfromtxt('winequality-white.csv',delimiter=\";\",skip_header=1)\n","\n","print(\"data:\", data.shape)\n","\n","# Prepare for proper training\n","np.random.shuffle(data) # randomly sort examples\n","\n","# take the first 3000 examples for training\n","# (remember array slicing from last week)\n","X_train = data[:3000,:11] # all features except last column\n","y_train = data[:3000,11]  # quality column\n","\n","# and the remaining examples for testing\n","X_test = data[3000:,:11] # all features except last column\n","y_test = data[3000:,11] # quality column\n","\n","print(\"First example:\")\n","print(\"Features:\", X_train[0])\n","print(\"Quality:\", y_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiwnyNHpzm9L"},"source":["# Problems\n","\n","\n","* First we want to understand the data better. Plot (`plt.hist`) the distribution of each of the features for the training data as well as the 2D distribution (either `plt.scatter` or `plt.hist2d`) of each feature versus quality. Also calculate the correlation coefficient (`np.corrcoef`) for each feature with quality. Which feature by itself seems most predictive for the quality?\n","\n","* Calculate the linear regression weights. Numpy provides functions for matrix multiplication (`np.matmul`), matrix transposition (`.T`) and matrix inversion (`np.linalg.inv`).\n","\n","* Use the weights to predict the quality for the test dataset. How\n","does your predicted quality compare with the true quality of the test data? Calculate the correlation coefficient between predicted and true quality and draw a scatter plot."]},{"cell_type":"markdown","source":["**Homework Submission**\n","\n","When you submit your exercise sheet, please alwasy do two things\n","\n","1) Generate a PDF of your iPython notebook. Submit this PDF through Studium\n","\n","2) Provide a link to your google colab notebook so that we can directly execute and test your code. To do that click on \"share\", change access to \"anyone with the link\", copy the link and add it as a comment to your submission on Studium. "],"metadata":{"id":"siD868R1oXgm"}},{"cell_type":"markdown","metadata":{"id":"-JyaU3ZDviXL"},"source":["# Hints"]},{"cell_type":"markdown","metadata":{"id":"k2_gi8izviXM"},"source":["Formally, we want to find weights $w_i$ that minimize:\n","$$\n","\\sum_{j}\\left(\\sum_{i} X_{i j} w_{i}-y_{j}\\right)^{2}\n","$$\n","The index $i$ denotes the different features (properties of the wines) while the index $j$ runs over the different wines. The matrix $X_{ij}$ contains the training data, $y_j$ is the 'true' quality for sample $j$. The weights can be found by taking the first derivative of the above expression with respect to the weights and setting it to zero (the standard strategy for finding an extremum), and solving the corresponding system of equations (for a detailed derivation, see [here](https://en.wikipedia.org/wiki/Ordinary_least_squares)). The result is:\n","$$\n","\\overrightarrow{\\mathbf{w}}=\\left(\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{T} \\overrightarrow{\\mathbf{y}}\n","$$\n","\n","In the end, you should have as many components of $w_i$ as there are features in the data (i.e. eleven in this case). \n","\n","You can use `.shape` to inspect the dimensions of numpy tensors.\n"]},{"cell_type":"markdown","source":["# Solution\n","\n","*link to the google colab*\n","\n","https://colab.research.google.com/drive/1Hrh-rII5lWHAk-ELYnX3SrpWPWS7KKrG?usp=sharing\n","\n","\n","**Understand the data**"],"metadata":{"id":"3PYrMOc0rAQm"}},{"cell_type":"code","source":["# initialize first maximum correlation coefficient\n","R_max = 0\n","\n","for i in range(X_train.shape[1]):\n","  # histogram\n","  plt.grid(True)\n","  plt.title(f'Histogram of feature {i}')\n","  plt.hist(X_train[:,i])\n","  plt.show()\n","\n","  # 2D-distribution\n","  plt.grid(True)\n","  plt.title(f'Quality versus Features {i}')\n","  plt.scatter(y_train,X_train[:,i])\n","  plt.show()\n","\n","  # correlation coefficient \n","  R = np.corrcoef([X_train[:,i],y_train], rowvar=True)[0,1]\n","  print(f'Correlation coefficient for features {i}: {R} \\n')\n","  if R > R_max:\n","    R_max = R\n","    R_max_i = i \n","\n","print(f'Feature {R_max_i} is the most predictive for the quality with the largest correlation coefficient')"],"metadata":{"id":"1APT586prkjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature 10, the \"alcohol\" is the most predictive for the quality according to correlation coefficient."],"metadata":{"id":"ta5XWcFxQo2k"}},{"cell_type":"markdown","source":["**Linear regression weight** "],"metadata":{"id":"8nvAomk9r1kh"}},{"cell_type":"code","source":["# linear regression weight calculation\n","W = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_train.T,X_train)),X_train.T),y_train)\n","print(f'Computed weights: {W} \\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"fkFQeNxXsBOZ","executionInfo":{"status":"ok","timestamp":1663231819233,"user_tz":-120,"elapsed":523,"user":{"displayName":"Antoine Dupuis","userId":"12168416720763859895"}},"outputId":"7d8043d1-86ae-49cb-b373-f892e921b826"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Computed weights: [-4.35236008e-02 -1.87732112e+00 -4.13907684e-02  2.15100416e-02\n"," -1.55361213e+00  6.17048936e-03 -9.79592577e-04  2.08298585e+00\n","  1.91936150e-01  3.51660176e-01  3.53290243e-01] \n","\n"]}]},{"cell_type":"markdown","source":["**Linear regression accuracy** "],"metadata":{"id":"f4ifhWCl3pYl"}},{"cell_type":"code","source":["# quality prediction with linear model using test data\n","y_predict = np.matmul(X_test,W)\n","y_predict2 = np.matmul(X_train,W)\n","\n","# cost function value with train dataset\n","C_train = np.sum(((y_predict2-y_train)**2))\n","print(f'loss for train dataset: {C_train} \\n')\n","\n","# correlation coefficient between predicted and true quality of train dataset\n","R_pred2 = np.corrcoef([y_train,y_predict2], rowvar=True)[0,1]\n","print(f'Correlation coefficient between predicted and true quality of train dataset: {R_pred2} \\n')\n","\n","# cost function value with test dataset\n","C = np.sum(((y_predict-y_test)**2))\n","print(f'loss for test dataset: {C} \\n')\n","\n","# correlation coefficient between predicted and true quality of test dataset\n","R_pred = np.corrcoef([y_test,y_predict], rowvar=True)[0,1]\n","print(f'Correlation coefficient between predicted and true quality of test dataset: {R_pred} \\n')\n","\n","# plot between predicted and true value\n","x = np.arange(1,11)\n","y = x\n","plt.grid(True)\n","plt.title('True versus Predicted quality')\n","plt.plot(x,y, c='k')\n","plt.scatter(y_test,y_predict)\n","plt.show()"],"metadata":{"id":"ssD59I3kFHDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plot prediction versus true quality gives a a good idea of the model accuracy. As a result, a perfect model would have points aligned with the black curve x = y. The further the point are, greater is the inaccuracy in prediction. Here one can conclude that the linear model isn't a good solution for this dataset."],"metadata":{"id":"3jMPhqNaPwPe"}}]}