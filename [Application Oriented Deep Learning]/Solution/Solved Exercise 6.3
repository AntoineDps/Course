{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19niaAtAycBVkFiMG1ctrTdwvZ-kcLx1I","timestamp":1665078786329},{"file_id":"https://github.com/DeepLearningForPhysicsResearchBook/deep-learning-physics/blob/main/Exercise_05_3.ipynb","timestamp":1632138392312}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WSW9BmePM7zZ"},"source":["# Exercise 6.3: Neural Networks in Keras"]},{"cell_type":"code","metadata":{"id":"HnpsfvafCzPS"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t02FemO-M7za"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# See https://keras.io/\n","# for extennsive documentation\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.models import Sequential\n","from keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IrvyHKHTM7ze"},"source":["Let us visit the problem of wine quality prediction previously encountered one final time. After linear regression and a self-made network, we can now explore the comfort provided by the Keras library."]},{"cell_type":"code","metadata":{"id":"-H-L5egsM7ze","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665150044704,"user_tz":-120,"elapsed":936,"user":{"displayName":"Antoine Dupuis","userId":"12168416720763859895"}},"outputId":"47fd19fb-e4e8-4901-dc2c-199b030459e2"},"source":["# The code snippet below is responsible for downloading the dataset to\n","# Google. You can directly download the file using the link\n","# if you work with a local anaconda setup\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-07 13:40:43--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 264426 (258K) [application/x-httpd-php]\n","Saving to: ‘winequality-white.csv’\n","\n","winequality-white.c 100%[===================>] 258.23K  1007KB/s    in 0.3s    \n","\n","2022-10-07 13:40:44 (1007 KB/s) - ‘winequality-white.csv’ saved [264426/264426]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"mDToshKJNWGY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665150044704,"user_tz":-120,"elapsed":2,"user":{"displayName":"Antoine Dupuis","userId":"12168416720763859895"}},"outputId":"ba7e3c64-df58-4e1e-b607-d1da11f34def"},"source":["# load all examples from the file\n","data = np.genfromtxt('winequality-white.csv',delimiter=\";\",skip_header=1)\n","\n","print(\"data:\", data.shape)\n","\n","# Prepare for proper training\n","np.random.shuffle(data) # randomly sort examples\n","\n","# take the first 3000 examples for training\n","X_train = data[:3000,:11] # all features except last column\n","y_train = data[:3000,11]  # quality column\n","\n","# and the remaining examples for testing\n","X_test = data[3000:,:11] # all features except last column\n","y_test = data[3000:,11] # quality column\n","\n","print(\"First example:\")\n","print(\"Features:\", X_train[0])\n","print(\"Quality:\", y_train[0])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data: (4898, 12)\n","First example:\n","Features: [6.800e+00 3.300e-01 3.000e-01 2.100e+00 4.700e-02 3.500e+01 1.470e+02\n"," 9.886e-01 3.240e+00 5.600e-01 1.340e+01]\n","Quality: 6.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"VXx8BXB_M7zi"},"source":["Below is the simple network from exercise 4.1 implemented using Keras. In addition to the network we define the loss function and optimiser."]},{"cell_type":"code","metadata":{"id":"Z0HMdw9eM7zi"},"source":["from tensorflow.python.ops.gen_math_ops import SqrtGrad\n","# See: https://keras.io/api/models/sequential/ and \n","# https://keras.io/api/layers/core_layers/dense/\n","# We can use the Sequential class to very easiliy\n","# build a simple architecture\n","model = Sequential()\n","# 11 inputs, 20 outputs, relu\n","model.add(Dense(20, input_dim=11, activation='relu')) \n","\n","# Add n more layers of 50 neurons\n","nbr_layers = 10\n","nbr_neurons = 20\n","for i in range (nbr_layers):\n","  model.add(Dense(nbr_neurons, activation='relu'))\n","\n","# 20 inputs (automatically detected by Keras), 1 output, linear activation\n","model.add(Dense(1, activation='linear'))\n","\n","# Add a momentum\n","#tf.keras.optimizers.SGD(nesterov=True)\n","\n","# Set loss function and optimiser algorithm\n","model.compile(loss='mse',  # mean squared error\n","              optimizer='adam'# stochastic gradient descent\n","             ) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I98jdZcqM7zm"},"source":["# Training and evaluation below\n","\n","The code below trains the network for 5 epochs using the loss function and optimiser defined above. Each example is individually passed to the network"]},{"cell_type":"code","metadata":{"id":"2eYJwWOOAVeN","executionInfo":{"status":"error","timestamp":1665150068034,"user_tz":-120,"elapsed":22699,"user":{"displayName":"Antoine Dupuis","userId":"12168416720763859895"}},"outputId":"24cd6c4c-268e-4052-e2db-fa058c2f02b2","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history = model.fit(X_train, y_train, \n","                    validation_data=(X_test, y_test),\n","                    epochs=30, batch_size=64)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","47/47 [==============================] - 3s 21ms/step - loss: 8.0091 - val_loss: 3.2586\n","Epoch 2/30\n","47/47 [==============================] - 0s 9ms/step - loss: 2.3836 - val_loss: 1.8247\n","Epoch 3/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.8862 - val_loss: 0.8435\n","Epoch 4/30\n","47/47 [==============================] - 1s 13ms/step - loss: 0.7469 - val_loss: 0.8612\n","Epoch 5/30\n","47/47 [==============================] - 1s 12ms/step - loss: 0.6508 - val_loss: 0.8185\n","Epoch 6/30\n","47/47 [==============================] - 0s 7ms/step - loss: 0.6504 - val_loss: 0.7500\n","Epoch 7/30\n","47/47 [==============================] - 0s 9ms/step - loss: 0.6330 - val_loss: 0.7200\n","Epoch 8/30\n","47/47 [==============================] - 1s 12ms/step - loss: 0.6101 - val_loss: 0.7825\n","Epoch 9/30\n","47/47 [==============================] - 0s 8ms/step - loss: 0.6301 - val_loss: 0.7373\n","Epoch 10/30\n","47/47 [==============================] - 1s 11ms/step - loss: 0.6213 - val_loss: 0.7422\n","Epoch 11/30\n","47/47 [==============================] - 0s 8ms/step - loss: 0.6048 - val_loss: 0.6858\n","Epoch 12/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.5970 - val_loss: 0.6793\n","Epoch 13/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.6041 - val_loss: 0.6995\n","Epoch 14/30\n","47/47 [==============================] - 0s 9ms/step - loss: 0.6244 - val_loss: 0.7180\n","Epoch 15/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.5783 - val_loss: 0.6738\n","Epoch 16/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.5898 - val_loss: 0.6694\n","Epoch 17/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.5881 - val_loss: 0.6856\n","Epoch 18/30\n","47/47 [==============================] - 1s 14ms/step - loss: 0.6217 - val_loss: 0.6800\n","Epoch 19/30\n","47/47 [==============================] - 0s 10ms/step - loss: 0.5861 - val_loss: 0.7019\n","Epoch 20/30\n","47/47 [==============================] - 0s 8ms/step - loss: 0.5807 - val_loss: 0.6602\n","Epoch 21/30\n","47/47 [==============================] - 0s 9ms/step - loss: 0.5744 - val_loss: 0.6640\n","Epoch 22/30\n","47/47 [==============================] - 0s 6ms/step - loss: 0.5979 - val_loss: 0.8907\n","Epoch 23/30\n","47/47 [==============================] - 0s 7ms/step - loss: 0.6057 - val_loss: 0.6518\n","Epoch 24/30\n","47/47 [==============================] - 0s 8ms/step - loss: 0.5762 - val_loss: 0.6749\n","Epoch 25/30\n","47/47 [==============================] - 0s 7ms/step - loss: 0.5900 - val_loss: 0.6677\n","Epoch 26/30\n","47/47 [==============================] - 0s 9ms/step - loss: 0.5706 - val_loss: 0.6677\n","Epoch 27/30\n","47/47 [==============================] - 0s 9ms/step - loss: 0.6124 - val_loss: 0.6585\n","Epoch 28/30\n","47/47 [==============================] - 0s 8ms/step - loss: 0.6055 - val_loss: 0.6411\n","Epoch 29/30\n","47/47 [==============================] - 1s 18ms/step - loss: 0.5659 - val_loss: 0.6562\n","Epoch 30/30\n","47/47 [==============================] - 0s 6ms/step - loss: 0.5807 - val_loss: 0.6472\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-49009c0080e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     epochs=30, batch_size=64)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"XOyiHcIkAVeO"},"source":["# The history object returned by the model training above \n","# contains the values of the loss function (the mean-squared-error)\n","# at different epochs\n","# We discard the first epoch as the loss value is very high,\n","# obscuring the rest of the distribution\n","train_loss = history.history[\"loss\"][1:]\n","test_loss = history.history[\"val_loss\"][1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XycXnZEqAVeP"},"source":["# Prepare and plot loss over time\n","plt.plot(train_loss,label=\"train\")\n","plt.plot(test_loss,label=\"test\")\n","plt.legend()\n","plt.xlabel(\"Epoch-1\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qORl2YivAVeQ"},"source":["# After the training:\n","\n","# Prepare scatter plot\n","y_pred = model.predict(X_test)[:,0]\n","print(y_pred)\n","\n","\n","print(\"Correlation coefficient:\", np.corrcoef(y_pred,y_test)[0,1])\n","plt.scatter(y_pred,y_test)\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFUQThOeAVeQ"},"source":["np.corrcoef(y_pred,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KI7zcK_uM7zp"},"source":["\n","# Problems\n","\n","* Use the notebook as starting point. It already contains the simple network from Exercise 4.1 implemented in Keras.\n","\n","* Currently, SGD is used without momentum. Try training with a momentum term. Replace SGD with the Adam optimizer and train using that. (See: https://keras.io/api/optimizers/)\n","* Add two more hidden layers to the network (you can choose the number of nodes but make sure to apply the ReLu activation function after each) and train again.\n","* Test differet numbers of examples (i.e. change the batch batch size) to be simulataneously used by the network. \n","* (bonus) optimize the network architecture to get the best correlation coefficient. (Let's see who gets the most out of the data). "]},{"cell_type":"code","metadata":{"id":"2mUTzx7h2mKJ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Answers\n","\n","1.   nan\n","2.   Using the Keras optimizer, a momentum is added to the SCG optimizer. Further, the Adam optimizer is chosen and its performance, with this network, isn't great when looking a the true value VS predicted value plot.\n","3.   Two more layers are added, each uses relu activation function and contain 50 neurons. After retraining a few times, the performance of the network  doesn't change much at all...\n","4.   Changing the Batch size improve slightly the performace of the network, however more epoch would be more relevant to find out the real potential impact of changing this parameter (batch size). The best results were found for batch size = 64\n","5.   Without talking about optimization, several structure have been used to try to lower the loss function values. For this tests, the number of epoch was taken up to 30.\n","The number of hidden neurons were ranging from 10 to 100, and number of layers, from 5 to 100. Results shows that deeper netwrok are not necessaraly more accurate. The lowest loss found was about 0.5 which isn't great results. This result was obtained with 5 layers of 20 neurons only.\n","\n","\n","\n"],"metadata":{"id":"tTtCzWHFpj-e"}},{"cell_type":"code","source":[],"metadata":{"id":"Nyn675oIpm_c"},"execution_count":null,"outputs":[]}]}
